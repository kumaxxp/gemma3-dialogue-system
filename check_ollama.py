#!/usr/bin/env python3
"""
Ollamaç’°å¢ƒè¨ºæ–­ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
æ¥ç¶šã¨ãƒ¢ãƒ‡ãƒ«ã®çŠ¶æ…‹ã‚’ç¢ºèª
"""

import sys
import json
import subprocess
from typing import Dict, List, Any

try:
    import ollama
    print("âœ… ollamaãƒ‘ãƒƒã‚±ãƒ¼ã‚¸: ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ")
except ImportError:
    print("âŒ ollamaãƒ‘ãƒƒã‚±ãƒ¼ã‚¸: æœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«")
    print("   å®Ÿè¡Œ: pip install ollama")
    sys.exit(1)

def check_ollama_service() -> bool:
    """Ollamaã‚µãƒ¼ãƒ“ã‚¹ã®çŠ¶æ…‹ç¢ºèª"""
    print("\n=== Ollamaã‚µãƒ¼ãƒ“ã‚¹ç¢ºèª ===")
    
    try:
        result = subprocess.run(
            ["systemctl", "is-active", "ollama"],
            capture_output=True,
            text=True
        )
        
        if result.stdout.strip() == "active":
            print("âœ… Ollamaã‚µãƒ¼ãƒ“ã‚¹: ç¨¼åƒä¸­")
            return True
        else:
            print(f"âš ï¸ Ollamaã‚µãƒ¼ãƒ“ã‚¹: {result.stdout.strip()}")
            return False
    except Exception as e:
        print(f"âš ï¸ ã‚µãƒ¼ãƒ“ã‚¹ç¢ºèªå¤±æ•—: {e}")
        return False

def check_ollama_connection() -> bool:
    """Ollama APIã¸ã®æ¥ç¶šç¢ºèª"""
    print("\n=== Ollama APIæ¥ç¶šç¢ºèª ===")
    
    try:
        # curlã§ã®ç¢ºèª
        result = subprocess.run(
            ["curl", "-s", "http://localhost:11434"],
            capture_output=True,
            text=True,
            timeout=5
        )
        if "Ollama" in result.stdout or result.returncode == 0:
            print("âœ… APIæ¥ç¶š: http://localhost:11434")
        else:
            print("âš ï¸ APIæ¥ç¶š: å¿œç­”ãªã—")
    except:
        print("âš ï¸ curlãƒ†ã‚¹ãƒˆå¤±æ•—")
    
    return True

def check_models() -> Dict[str, Any]:
    """ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆã®ç¢ºèª"""
    print("\n=== ãƒ¢ãƒ‡ãƒ«ç¢ºèª ===")
    
    models_info = {
        "method": None,
        "models": [],
        "raw_response": None
    }
    
    # æ–¹æ³•1: ollama.list()ã‚’è©¦ã™
    try:
        response = ollama.list()
        models_info["raw_response"] = str(type(response))
        
        print(f"ollama.list()ã®è¿”ã‚Šå€¤å‹: {type(response)}")
        
        # æ§˜ã€…ãªå½¢å¼ã«å¯¾å¿œ
        if isinstance(response, dict):
            print(f"  ã‚­ãƒ¼: {list(response.keys())}")
            
            # 'models'ã‚­ãƒ¼ãŒã‚ã‚‹å ´åˆ
            if 'models' in response:
                models_info["method"] = "dict with 'models' key"
                for model in response['models']:
                    if isinstance(model, dict):
                        name = model.get('name', 'unknown')
                        models_info["models"].append(name)
                        print(f"  - {name}")
            # ç›´æ¥ãƒ¢ãƒ‡ãƒ«æƒ…å ±ãŒå…¥ã£ã¦ã„ã‚‹å ´åˆ
            else:
                models_info["method"] = "dict without 'models' key"
                print(f"  å†…å®¹: {json.dumps(response, indent=2, default=str)[:500]}")
        
        elif isinstance(response, list):
            models_info["method"] = "list"
            for item in response:
                if isinstance(item, dict):
                    name = item.get('name', str(item))
                    models_info["models"].append(name)
                    print(f"  - {name}")
                else:
                    models_info["models"].append(str(item))
                    print(f"  - {item}")
        
        else:
            models_info["method"] = f"unknown type: {type(response)}"
            print(f"  äºˆæœŸã—ãªã„å‹: {response}")
            
    except Exception as e:
        print(f"âŒ ollama.list()ã‚¨ãƒ©ãƒ¼: {e}")
        models_info["method"] = f"error: {e}"
    
    # æ–¹æ³•2: CLIã‚³ãƒãƒ³ãƒ‰ã‚’è©¦ã™
    print("\n--- CLIã§ã®ç¢ºèª ---")
    try:
        result = subprocess.run(
            ["ollama", "list"],
            capture_output=True,
            text=True,
            timeout=5
        )
        
        if result.returncode == 0:
            print("CLIå‡ºåŠ›:")
            lines = result.stdout.strip().split('\n')
            for line in lines[:10]:  # æœ€åˆã®10è¡Œ
                print(f"  {line}")
            
            # ãƒ¢ãƒ‡ãƒ«åã‚’æŠ½å‡ºï¼ˆé€šå¸¸ã¯2è¡Œç›®ä»¥é™ï¼‰
            for line in lines[1:]:
                parts = line.split()
                if parts:
                    model_name = parts[0]
                    if model_name not in models_info["models"]:
                        models_info["models"].append(f"[CLI] {model_name}")
        else:
            print(f"âš ï¸ CLIã‚¨ãƒ©ãƒ¼: {result.stderr}")
            
    except Exception as e:
        print(f"âš ï¸ CLIç¢ºèªå¤±æ•—: {e}")
    
    return models_info

def test_gemma3() -> bool:
    """Gemma3ãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆ"""
    print("\n=== Gemma3å‹•ä½œãƒ†ã‚¹ãƒˆ ===")
    
    models_to_test = ["gemma3:4b", "gemma3", "gemma2:2b"]  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    
    for model in models_to_test:
        print(f"\nãƒ†ã‚¹ãƒˆ: {model}")
        try:
            response = ollama.chat(
                model=model,
                messages=[
                    {"role": "user", "content": "Say 'OK' in one word"}
                ],
                options={
                    "num_predict": 5,
                    "temperature": 0.1
                }
            )
            
            if response and 'message' in response:
                content = response['message'].get('content', '')
                print(f"âœ… {model}: å‹•ä½œç¢ºèªOK")
                print(f"   å¿œç­”: {content[:50]}")
                return True
            else:
                print(f"âš ï¸ {model}: å¿œç­”å½¢å¼ãŒä¸æ­£")
                
        except Exception as e:
            print(f"âŒ {model}: {str(e)[:100]}")
    
    return False

def print_summary(models_info: Dict[str, Any], gemma_ok: bool):
    """è¨ºæ–­çµæœã®ã‚µãƒãƒªãƒ¼"""
    print("\n" + "="*50)
    print("è¨ºæ–­çµæœã‚µãƒãƒªãƒ¼")
    print("="*50)
    
    if models_info["models"]:
        print(f"âœ… æ¤œå‡ºã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«æ•°: {len(models_info['models'])}")
        
        gemma_models = [m for m in models_info["models"] if 'gemma' in m.lower()]
        if gemma_models:
            print(f"âœ… Gemmaãƒ¢ãƒ‡ãƒ«: {', '.join(gemma_models)}")
        else:
            print("âš ï¸ Gemmaãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            print("\næ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:")
            print("  ollama pull gemma3:4b")
    else:
        print("âŒ ãƒ¢ãƒ‡ãƒ«ãŒæ¤œå‡ºã§ãã¾ã›ã‚“")
        print("\næ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:")
        print("  1. ollama list ã§ç¢ºèª")
        print("  2. ollama pull gemma3:4b")
    
    if gemma_ok:
        print("\nâœ… Gemma3ã¯å‹•ä½œå¯èƒ½ã§ã™")
    else:
        print("\nâš ï¸ Gemma3ã®å‹•ä½œç¢ºèªãŒã§ãã¾ã›ã‚“")
    
    print("\nè©³ç´°æƒ…å ±:")
    print(f"  æ¤œå‡ºæ–¹æ³•: {models_info['method']}")
    print(f"  å¿œç­”å‹: {models_info['raw_response']}")

def main():
    print("ğŸ” Ollamaç’°å¢ƒè¨ºæ–­ãƒ„ãƒ¼ãƒ«")
    print("="*50)
    
    # 1. ã‚µãƒ¼ãƒ“ã‚¹ç¢ºèª
    service_ok = check_ollama_service()
    
    if not service_ok:
        print("\nğŸ’¡ Ollamaã‚’èµ·å‹•ã—ã¦ãã ã•ã„:")
        print("   sudo systemctl start ollama")
        print("   ã¾ãŸã¯")
        print("   ollama serve")
    
    # 2. æ¥ç¶šç¢ºèª
    check_ollama_connection()
    
    # 3. ãƒ¢ãƒ‡ãƒ«ç¢ºèª
    models_info = check_models()
    
    # 4. Gemma3ãƒ†ã‚¹ãƒˆ
    gemma_ok = test_gemma3()
    
    # 5. ã‚µãƒãƒªãƒ¼
    print_summary(models_info, gemma_ok)
    
    # çµ‚äº†ã‚³ãƒ¼ãƒ‰
    if gemma_ok:
        print("\nâœ… è¨ºæ–­å®Œäº†: å•é¡Œãªã—")
        return 0
    else:
        print("\nâš ï¸ è¨ºæ–­å®Œäº†: è¦å¯¾å‡¦")
        return 1

if __name__ == "__main__":
    sys.exit(main())